{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%run mathutil.ipynb\r\n",
    "\r\n",
    "np.random.seed(1234)\r\n",
    "def randomize(): np.random.seed(time.time())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class Model(object):\r\n",
    "    def __init__(self, name, dataset):\r\n",
    "        self.name = name\r\n",
    "        self.dataset = dataset\r\n",
    "        self.is_training = False\r\n",
    "        if not hasattr(self, 'rand_std'): self.rand_std = 0.030\r\n",
    "\r\n",
    "    def __str__(self):\r\n",
    "        return '{}/{}'.format(self.name, self.dataset)\r\n",
    "\r\n",
    "    def exec_all(self, epoch_count=10, batch_size=10, learning_rate=0.001, report=0, show_cnt=3):\r\n",
    "        self.train(epoch_count, batch_size, learning_rate, report)\r\n",
    "        self.test()\r\n",
    "        if show_cnt > 0: self.visualize(show_cnt)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class MlpModel(Model):\r\n",
    "    def __init__(self, name, dataset, hconfigs):\r\n",
    "        super(MlpModel, self).__init__(name, dataset)\r\n",
    "        self.init_parameters(hconfigs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def mlp_init_parameters(self, hconfigs):\r\n",
    "    self.hconfigs = hconfigs\r\n",
    "    self.pm_hiddens = []\r\n",
    "\r\n",
    "    prev_shape = self.dataset.input_shape\r\n",
    "\r\n",
    "    for hconfig in hconfigs:\r\n",
    "        pm_hidden, prev_shape = self.alloc_layer_param(prev_shape, hconfig)\r\n",
    "        self.pm_hiddens.append(pm_hidden)\r\n",
    "\r\n",
    "    output_cnt = int(np.prod(self.dataset.output_shape))\r\n",
    "    self.pm_output, _ = self.alloc_layer_param(prev_shape, output_cnt)\r\n",
    "\r\n",
    "def mlp_alloc_layer_param(self, input_shape, hconfig):\r\n",
    "    input_cnt = np.prod(input_shape)\r\n",
    "    output_cnt = hconfig\r\n",
    "\r\n",
    "    weight, bias = self.alloc_param_pair([input_cnt, output_cnt])\r\n",
    "\r\n",
    "    return {'w': weight, 'b': bias}, output_cnt\r\n",
    "\r\n",
    "def mlp_alloc_param_pair(self, shape):\r\n",
    "    weight = np.random.normal(0, self.rand_std, shape)\r\n",
    "    bias = np.zeros([shape[-1]])\r\n",
    "    return weight, bias\r\n",
    "\r\n",
    "MlpModel.init_parameters = mlp_init_parameters\r\n",
    "MlpModel.alloc_layer_param = mlp_alloc_layer_param\r\n",
    "MlpModel.alloc_param_pair = mlp_alloc_param_pair"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def mlp_model_train(self, epoch_count=10, batch_size=10, learning_rate=0.001, report=0):\r\n",
    "    self.learning_rate = learning_rate\r\n",
    "\r\n",
    "    batch_count = int(self.dataset.train_count / batch_size)\r\n",
    "    time1 = time2 = int(time.time())\r\n",
    "    if report != 0:\r\n",
    "        print('Model {} train started:'.format(self.name))\r\n",
    "\r\n",
    "    for epoch in range(epoch_count):\r\n",
    "        costs = []\r\n",
    "        accs = []\r\n",
    "        self.dataset.shuffle_train_data(batch_size*batch_count)\r\n",
    "        for n in range(batch_count):\r\n",
    "            trX, trY = self.dataset.get_train_data(batch_size, n)\r\n",
    "            cost, acc = self.train_step(trX, trY)\r\n",
    "            costs.append(cost)\r\n",
    "            accs.append(acc)\r\n",
    "\r\n",
    "        if report > 0 and (epoch+1) % report == 0:\r\n",
    "            vaX, vaY = self.dataset.get_valid_data(100)\r\n",
    "            acc = self.eval_accuracy(vaX, vaY)\r\n",
    "            time3 = int(time.time())\r\n",
    "            tm1, tm2 = time3 - time2, time3 - time1\r\n",
    "            self.dataset.train_prt_result(epoch+1, costs, accs, acc, tm1, tm2)\r\n",
    "            time2 = time3\r\n",
    "\r\n",
    "        tm_total = int(time.time()) - time1\r\n",
    "        print('Model {} train ended in {} secs:'.format(self.name, tm_total))\r\n",
    "\r\n",
    "MlpModel.train = mlp_model_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def mlp_model_test(self):\r\n",
    "    teX, teY = self.dataset.get_test_data()\r\n",
    "    time1 = int(time.time())\r\n",
    "    acc = self.eval_accuracy(teX, teY)\r\n",
    "    time2 = int(time.time())\r\n",
    "    self.dataset.test_prt_result(self.name, acc, time2-time1)\r\n",
    "\r\n",
    "MlpModel.test = mlp_model_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def mlp_model_visualize(self, num):\r\n",
    "    print('Model {} Visualization'.format(self.name))\r\n",
    "    deX, deY = self.dataset.get_visualize_data(num)\r\n",
    "    est = self.get_estimate(deX)\r\n",
    "    self.dataset.visualize(deX, est, deY)\r\n",
    "\r\n",
    "MlpModel.visualize = mlp_model_visualize"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def mlp_train_step(self, x, y):\r\n",
    "    self.is_training = True\r\n",
    "\r\n",
    "    output, aux_nn = self.forward_neuralnet(x)\r\n",
    "    loss, aux_pp = self.forward_postproc(output, y)\r\n",
    "    accuracy = self.eval_accuracy(x, y, output)\r\n",
    "\r\n",
    "    G_loss = 1.0\r\n",
    "    G_output = self.backprop_postproc(G_loss, aux_pp)\r\n",
    "    self.backprop_postproc(G_output, aux_nn)\r\n",
    "\r\n",
    "    self.is_training = False\r\n",
    "\r\n",
    "    return loss, accuracy\r\n",
    "\r\n",
    "MlpModel.train_step = mlp_train_step"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def mlp_forward_neuralnet(self, x):\r\n",
    "    hidden = x\r\n",
    "    aux_layers = []\r\n",
    "\r\n",
    "    for n, hconfig in enumerate(self.hconfigs):\r\n",
    "        hidden, aux = self.forward_layer(hidden, hconfig, self.pm_hiddens[n])\r\n",
    "        aux_layers.append(aux)\r\n",
    "\r\n",
    "    output, aux_out = self.forward_layer(hidden, None, self.pm_output)\r\n",
    "\r\n",
    "    return output, [aux_out, aux_layers]\r\n",
    "\r\n",
    "def mlp_backprop_neuralnet(self, G_output, aux):\r\n",
    "    aux_out, aux_layers = aux\r\n",
    "\r\n",
    "    G_hidden = self.backprop_layer(G_output, None, self.pm_output, aux_out)\r\n",
    "\r\n",
    "    for n in reversed(range(len(self.hconfigs))):\r\n",
    "        hconfig, pm, aux = self.hconfigs[n], self.pm_hiddens[n], aux_layers[n]\r\n",
    "        G_hidden = self.backprop_layer(G_hidden, hconfig, pm, aux)\r\n",
    "\r\n",
    "    return G_hidden\r\n",
    "\r\n",
    "MlpModel.forward_neuralnet = mlp_forward_neuralnet\r\n",
    "MlpModel.backprop_neuralnet = mlp_backprop_neuralnet"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.10"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('cv': conda)"
  },
  "interpreter": {
   "hash": "101ba9b0878a37f24d322c2ef0e0a38d9299b41eadd5803319bb00780a61a59d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}